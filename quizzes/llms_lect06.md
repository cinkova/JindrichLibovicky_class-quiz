# Large Language Models: Lecture 6 Quiz

## What order of magnitude is the size of contemporary corpora for (pre-)training language models?

1. thousands of tokens
2. millions of tokens
3. billions of tokens
4. (X) trillions of tokens
5. quadrillions of tokens

## Where can one commonly find datasets for NLP tasks?
1. Only from proprietary sources
2. Exclusively through direct data collection efforts
3. (X) Various sources such as online repositories, academic institutions, and web scraping
4. Solely from government agencies


## What are some challenges associated with validating the performance of Natural Language Processing (NLP) models?
1. Subjectivity in human evaluation
2. Limited availability of labeled data for testing
3. Difficulty in defining universal evaluation metrics
4. (X) All of the above


## In NLP, what role do pre-training datasets serve in model development?
1. They provide labeled data for specific tasks
2. (X) They enable models to learn general language representations
3. They serve as test datasets for evaluating model performance
4. They are used primarily for fine-tuning existing models


## What is a potential drawback of relying solely on automatic evaluation metrics such as BLEU or ROUGE in assessing the performance of Natural Language Processing models?
1. (X) Automatic metrics may not capture the nuanced aspects of language understanding, leading to discrepancies between metric scores and human judgments.
2. Automatic metrics often require large computational resources, making them impractical for real-time evaluation of NLP models.
3. Automatic metrics tend to favor models with higher computational complexity, biasing the evaluation towards more resource-intensive approaches.
4. Automatic metrics may overlook the impact of data preprocessing techniques on model performance, resulting in inaccurate assessments of NLP systems.
