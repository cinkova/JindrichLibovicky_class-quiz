# Large Language Models: Lecture 4 Recap (Inference)

## What does it mean when a model is described as "autoregressive"?

1. The model predicts the regression coefficients automatically.
2. The model generates text by conditioning on the entire dataset.
3. (X) The model generates each token by conditioning on the previously generated tokens.
4. The model regresses towards the mean of the training data.

## What is beam search used for in the context of language models?

1. To reduce the computational complexity of the model.
2. To increase the randomness of the text generation.
3. To decrease the diversity of the generated text.
4. (X) To find more likely sequences of text by keeping track of a fixed number of hypotheses.

## Which of the following best describes "top-$k$ sampling"?

1. Sampling from the $k$ **most recent tokens** generated by the model.
2. Sampling from the **entire vocabulary**, then filtering out the top $k$ tokens.
3. (X) Sampling from the $k$ **most likely** next tokens at each step of the generation.
4. Sampling from the **least likely** $k$ tokens to increase output variability.

## How does beam search differ from greedy search?

1. (X) Beam search keeps multiple hypotheses at each step, while greedy search always picks the single best next token.
2. Beam search generates text faster than greedy search.
3. Beam search always produces more coherent text than greedy search.
4. Greedy search can backtrack to previous states, but beam search cannot.

## What problem does nucleus sampling solve that is inherent to top-k sampling?

1. It makes the model faster by reducing the number of tokens considered at each step.
2. (X) It dynamically adjusts the number of tokens considered based on their cumulative probability, leading to more natural text generation.
3. It exclusively focuses on the most likely token, thus making the text more predictable.
4. It generates text that is less coherent but more diverse.
