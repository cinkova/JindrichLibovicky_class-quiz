<quiz title="Intro to ML: Lecture 10 Recap (Gradient Boosted Decision Trees)">

    <question>
        <text>One step in second-order minimization of function $f(x)$ is</text>

        <answer>$\alpha f'(x)$ where $\alpha$ is the learning rate, which is a hyperparameter.</answer>
        <answer>$\alpha f''(x)$ where $\alpha$ is the learning rate, which is not a hyperparameter.</answer>
        <answer correct="true">$-f'(x) / f''(x)$</answer>
        <answer>$f''(x) / f'(x)$</answer>
    </question>

    <question>
        <text>When we apply the Taylor on the loss function $\ell (t_i, y^{(t-1)}(\boldsymbol{x}_i) + y_t(\boldsymbol x_i))$ we get (the $'$ denotes taking derivative by $y^{(t-1)}(\boldsymbol{x}_i)$)</text>

        <answer correct="true">$\ell (t_i, y^{(t-1)}(\boldsymbol{x}_i)) + \ell'(t_i, y^{(t-1)}(\boldsymbol{x}_i)) y_t(\boldsymbol x_i) + \frac{1}{2} \ell''(t_i, y^{(t-1)}(\boldsymbol{x}_i)) y_t(\boldsymbol x_i)^2$</answer>
        <answer>$\ell (t_i, y^{(t-1)}(\boldsymbol{x}_i)) + \ell'(t_i, y^{(t-1)}(\boldsymbol{x}_i)) y_t(\boldsymbol x_i) + \ell''(t_i, y^{(t-1)}(\boldsymbol{x}_i)) y_t(\boldsymbol x_i)$</answer>
        <answer>$\ell (t_i, y^{(t-1)}(\boldsymbol{x}_i)) + \ell'(t_i, y^{(t-1)}(\boldsymbol{x}_i)) + \frac{1}{2} \ell''(t_i, y^{(t-1)}(\boldsymbol{x}_i))^2$</answer>
        <answer>$\ell (t_i, y^{(t-1)}(\boldsymbol{x}_i)) + 2\ell'(t_i, y^{(t-1)}(\boldsymbol{x}_i)) \ell''(t_i, y^{(t-1)}(\boldsymbol{x}_i)) y_t(\boldsymbol x_i)$</answer>
    </question>

    <question>
        <text>The splitting criterion in gradient boosted decision trees is</text>

        <answer>the sum of squared errors.</answer>
        <answer correct="true">a negative fraction of the first and second derivative of the loss function (when neglecting the regularization term).</answer>
        <answer>a negative fraction of the first derivative of the loss function (when neglecting the regularization term).</answer>
        <answer>the sum of the first three terms in the Taylor expansion of the loss function.</answer>

    </question>

    <question>
        <text>Gradient boosted decision trees for binary classification</text>

        <answer correct="true">first predict the logit and then applies the sigmoid function.</answer>
        <answer>predict the probability in the first tree and real-valued correction in the following trees.</answer>
        <answer>are different from gradient boosted decision trees for regression. We still use mean squared error, the splitting criterion becomes the Gini index.</answer>
        <answer>are computationally more efficient than gradient boosted decision trees for regression.</answer>

    </question>

    <question>
        <text>Gradient boosted decision trees for multi-class classification</text>

        <answer>are not possible, each tree can only predict a binary class.</answer>
        <answer correct="true">use a separate sequence of trees for each class logit, which are then normalized to probabilities using softmax.</answer>
        <answer>use a single sequence of trees, where node predict a categorical distribution over classes.</answer>
        <answer>use sequence of tree sequences: each set of tree does one-vs-all classification.</answer>
    </question>

    <question>
        <text>Select a **false** statement about choosing a suitable model for a task.</text>

        <answer>Gradient boosted decision trees are well suited for tabular data.</answer>
        <answer correct="true">Gradient boosted decision trees are well suited for image data because each tree can capture a different combination of pixel values.</answer>
        <answer>Multilayer perceptrons are well suited for high-dimensional data because fully connected layers can capture complex interactions between features.</answer>
        <answer>Ensembling can almost always improve the performance of a single model.</answer>
    </question>

</quiz>
