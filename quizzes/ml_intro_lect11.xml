<quiz title="Intro to ML: Lecture 11 Recap (SVD, PCA, k-Means)">

    <question>
        <text>Select a **false** statement about SVD $\boldsymbol X = \boldsymbol U \boldsymbol \Sigma \boldsymbol V^T$ of dimension $m \times n$ ?</text>

        <answer>$\boldsymbol U$ and $\boldsymbol V$ are orthonormal matrices.</answer>
        <answer>$\boldsymbol \Sigma$ is a diagonal matrix.</answer>
        <answer>The columns of $\boldsymbol U$ are the eigenvectors of $\boldsymbol X \boldsymbol X^T$.</answer>
        <answer correct="true">$\boldsymbol U^{-1} = \boldsymbol V^T$.</answer>
    </question>

    <question>
        <text>The Eckart-Young theorem states that</text>

        <answer correct="true">the best rank-$k$ approximation of $\boldsymbol X$ is given by $\boldsymbol U_k \boldsymbol \Sigma_k \boldsymbol V_k^T$ where $\boldsymbol U_k$ is the first $k$ columns of $\boldsymbol U$.</answer>
        <answer>Removing the smallest singular values of $\boldsymbol X$ will not change the Frobenius norm of $\boldsymbol X$.</answer>
        <answer>Removing the smallest singular values of $\boldsymbol X$ will remove the most variance from $\boldsymbol X$.</answer>
        <answer>Keeping any $k$ components from SVD will give the same rank-$k$ approximation of $\boldsymbol X$.</answer>

    </question>

    <question>
        <text>Select a **false** statement about PCA.</text>

        <answer>We cat get the principal components by taking the eigenvectors of the covariance matrix.</answer>
        <answer>We can get the principal components by doing SVD of the mean-centered data matrix.</answer>
        <answer>The principal components are orthogonal.</answer>
        <answer correct="true">The number of principal components is equal to the number of features.</answer>
    </question>

    <question>
        <text>PCA is a method for</text>

        <answer>dimensionality reduction.</answer>
        <answer>data compression.</answer>
        <answer>data visualization.</answer>
        <answer correct="true">all of the above.</answer>
    </question>

    <question>
        <text>The power iteration algorithm computes the eigenvector with the highest eigenvalue</text>

        <answer correct="true">multiplying the matrix by itself until convergence, the eigenvector is then the average of the columns of the resulting matrix.</answer>
        <answer>iteratively multiplying the matrix by a random vector.</answer>
        <answer>computing SVD of different powers of the matrix.</answer>
        <answer correct="true">iteratively multiplying a random vector by the matrix and normalizing the result.</answer>

    </question>

    <question>
        <text>The two stages of $k$-means clustering are</text>

        <answer>assigning points to clusters (to the nearest centers) and updating cluster sizes.</answer>
        <answer correct="true">assigning points to clusters and updating cluster centers.</answer>
        <answer>assigning points to clusters and rescaling the data, so the centers are orthogonal.</answer>
        <answer>assigning points to clusters and updating cluster sizes, so each cluster has the same number of points regardless of the distance.</answer>

    </question>

    <question>
        <text>The $k$-means clustering algorithm</text>

        <answer>always converges to the global optimum.</answer>
        <answer correct="true">converges to a local optimum.</answer>
        <answer>converges to the same optimum regardless of the initialization.</answer>
        <answer>hierarchically joins nearest clusters.</answer>

    </question>

</quiz>
