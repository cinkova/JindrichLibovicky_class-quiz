<quiz title="Intro to ML: Lecture 9 Recap (Decision Trees, Random Forests)">

    <question>
        <text>Select a **false stament** about training a decision tree.</text>

        <answer>We start with a single node.</answer>
        <answer>The training data are stored in the tree leaves.</answer>
        <answer>At each step, we split the data in the current node into two subsets based on a criterion.</answer>
        <answer correct="true">The goal of the training is to minimize the number of splits.</answer>
    </question>

    <question>
        <text>When building a decision tree for regression, we use the following criterion to split the data $I_\mathcal{T}$ in a node $\mathcal{T}$:</text>

        <answer>$\operatorname{Var}(I_\mathcal{T})$</answer>
        <answer correct="true">$|I_\mathcal{T}| \operatorname{Var}(I_\mathcal{T})$</answer>
        <answer>$\operatorname{Var}(I_\mathcal{T}) / |I_\mathcal{T}|$</answer>
        <answer>$\sum_{i \in I_\mathcal{T}} t_i^2$</answer>
    </question>

    <question>
        <text>The relationship between the loss function and the splitting criterion is:</text>

        <answer>The loss function is the splitting criterion.</answer>
        <answer correct="true">The splitting criterion is the lower bound of the loss function given the tree structure.</answer>
        <answer>Splitting criterion is the derivative of the loss function.</answer>
        <answer>The loss function is the derivative of the splitting criterion.</answer>
        <answer>There is no relationship, they are independent hyperparameters.</answer>

    </question>

    <question>
        <text>Optimizing negative log-likelihood of the correct class in a categorical distribution leads to</text>

        <answer correct="true">The entropy criterion $-|I_\mathcal{T}|\sum_{\text{classes }k}p_k \log p_k$.</answer>
        <answer>The cross-entropy criterion $-|I_\mathcal{T}|\sum_{\text{classes }k}p_k \log q_k$, where $q$ is an empirical distribution over the whole training set.</answer>
        <answer>The Gini impurity criterion $|I_\mathcal{T}|\sum_{\text{classes }k}p_k (1-p_k)$.</answer>
        <answer>An NP-hard problem, so we only predict a single class, instead of a probability distribution.</answer>

    </question>

    <question>
        <text>Select a **false statement** about random forests.</text>

        <answer>We use bagging to sample the training data independently for each tree in the forest.</answer>
        <answer>When splitting a node, we use a random subset of features.</answer>
        <answer>We use the same splitting criterion as for decision trees.</answer>
        <answer>The final prediction is the average of the predictions of all trees.</answer>
        <answer correct="true">Random forests are more prone to overfitting than decision trees.</answer>

    </question>

    <question>
        <text>The difference between random forests and gradient boosted decision trees is:</text>

        <answer>Random forests use a single decision tree, while gradient boosted decision trees use multiple decision trees.</answer>
        <answer correct="true">Random forests make independent predictions, while gradient boosted decision trees make predictions sequentially.</answer>
        <answer>Random forests use a single loss function, while gradient boosted decision trees use multiple loss functions.</answer>
        <answer>Random forests use random splits, while gradient boosted decision trees use deterministic splits.</answer>

    </question>

</quiz>
