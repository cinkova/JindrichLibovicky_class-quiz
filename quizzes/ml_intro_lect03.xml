<quiz title="Intro to ML: Lecture 3 Recap">

	<question>
		<text>Two sets of $D$-dimensional points $\boldsymbol{X_1}$ and $\boldsymbol{X_2}$ are linearly separable</text>

		<answer correct="true">If there exist $\boldsymbol{w}$, such that $\boldsymbol{x}^T\boldsymbol{w} > 0$ for each $\boldsymbol{x} \in \boldsymbol{X}_1$ and $\boldsymbol{x}^T\boldsymbol{w} &lt; 0$ for each $\boldsymbol{x} \in \boldsymbol{X}_2$.</answer>
		<answer>If there exist $\boldsymbol{w}$ such that all points from $\boldsymbol{X}_1$ and $\boldsymbol{X}_2$ lie on a hyperplane defined by $\boldsymbol{x}^T\boldsymbol{w}$.</answer>
		<answer>If there exist a projection matrix $\boldsymbol{W}$ such that for each $\boldsymbol{x} \in \boldsymbol{X}_1$ there exists $\bar{\boldsymbol{x}} \in \boldsymbol{X}_2$, such that $\boldsymbol{x}\boldsymbol{W} = \bar{\boldsymbol{x}}$</answer>
		<answer>If linear regressions fit for $\boldsymbol{X}_1$ and $\boldsymbol{X}_2$ have the same parameters.</answer>
	</question>

	<question>
		<text>Select a false claim about Bernoulli distribution.</text>

		<answer>Bernoulli distribution has two outcomes with probabilities $\varphi$ and $1 - \varphi$.</answer>
		<answer>$P(x) = \varphi^x(1 - \varphi)^{1-x}$.</answer>
		<answer correct="true">The entropy of Bernoulli distribution can never be zero.</answer>
		<answer>Bernoulli distribution has zero entropy if $\varphi = 0$ or $\varphi = 1$.</answer>
	</question>


	<question>
		<text>Entropy of a discrete random variable $X$</text>

		<answer>Is only defined if all outcomes have non-zero probability.</answer>
		<answer>Can be both positive and negative.</answer>
		<answer correct="true">Is an expectation of $\log P(x)$.</answer>
		<answer>Is a real number between zero and $e$ (if we use natural logarithm).</answer>
	</question>

	<question>
		<text>KL divergence of distributions $P$ and $Q$</text>

		<answer>Is a difference between two cross-entropies: $H(P, Q) - H(Q, P)$.</answer>
		<answer correct="true">Has the following interpretation: It says how many extra nats we need if we encoded messages from distribution $P$ using an encoding for distribution $Q$ compared to the optimum encoding.</answer>
		<answer>Can be both positive and negative because of Gibbs inequality.</answer>
		<answer>Is metric function because it is reflexive, symmetric and follows the triangular inequality.</answer>
	</question>

	<question>
		<text>Maximum likelihood estimation implies</text>

		<answer>Finding the most probable training data for given parameters.</answer>
		<answer>Maximizing the entropy of the training data, such that the model does make unnecessary assumptions about the data.</answer>
		<answer>Classification with logistic regression.</answer>
		<answer correct="true">Finding parameters that maximize the probability of the training data given the model.</answer>
	</question>

	<question>
		<text>Select a false statement about logistic regression.</text>

		<answer>It models binary classification using Bernoulli distribution.</answer>
		<answer>$P(C_1|\boldsymbol{x},\boldsymbol{w}) = \frac{1}{1 - \exp(-\boldsymbol{x}^\boldsymbol{w})}$, $P(C_0 | \boldsymbol{x}, \boldsymbol{w}) = 1 - P(C_1| \boldsymbol{x}, \boldsymbol{w})$.</answer>
		<answer>With fixed parameters $\boldsymbol{w}$, it leads to the same classification as the perceptron model.</answer>
		<answer correct="true">It is a results of applying the maximum likelihood estimation when we assume that the errors are normally distributed.</answer>

	</question>
</quiz>
