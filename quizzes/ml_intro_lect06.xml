<quiz title="Intro to ML: Lecture 6 Recap">
	<question>
		<text>The formula for TF-IDF is</text>

		<answer>$\operatorname{TF-IDF}(t, d) = P(t|d) \log \frac{1}{P(d | t)}$</answer>
		<answer>$\operatorname{TF-IDF}(t, d) = -P(t|d) \log \frac{P(d)}{P(d | t)}$</answer>
		<answer>$\operatorname{TF-IDF}(t, d) = P(t|d) \left( H(\mathcal{D}) - H(\mathcal{D}|t) \right)$</answer>
		<answer correct="true">$\operatorname{TF-IDF}(t, d) = P(t|d) \log \frac{P(d)}{P(d | t)}$</answer>

	</question>

	<question>
		<text>One of the reasons for logarithming the inverse document frequency ($\frac{|\mathcal{D}|}{|d \in \mathcal{D}: t|}$) is</text>

		<answer>It turns IDF into self-information of conditional distribution $P(d | t \in d)$, i.e., choosing document $d$ given that we know that it contains the term $t$.</answer>
		<answer correct="true">Word frequencies decrease quickly with word rank in language, it would be extremely high for low-frequency words.</answer>
		<answer>It ensures that the $L^2$-norm of the TF-IDF vector is always one.</answer>
		<answer>It makes the computation faster: the logarithm of fraction is a difference of logarithms.</answer>
	</question>

	<question>
		<text>We say that neural networks represent words as vectors because</text>

		<answer>Of the famous metaphor with vectors lying in a bed, i.e., embedded, therefore word embeddings.</answer>
		<answer>The activations on hidden layers are vectors.</answer>
		<answer correct="true">They correspond to columns of a weight matrix when the words would be represented as one-hot vectors assuming a finite vocabulary.</answer>
		<answer>The weight matrix is much smaller than if we used one-hot vectors with the same vocabulary size.</answer>
	</question>

	<question>
		<text>Language models</text>

		<answer correct="true">Estimate a probability of a token (word, character, ...) given context.</answer>
		<answer>Have no probabilistic interpretability and are only used to get word embeddings.</answer>
		<answer>Are any neural networks that process text.</answer>
		<answer>Are neural networks that have word embeddings both as an input and as an output.</answer>
	</question>

	<question>
		<text>The skip-gram model with embedding matrix $\boldsymbol{E}$ and output matrix $\boldsymbol{W}$.</text>

		<answer>Predicts a probability of two words appearing after each other as $\sigma\left((\boldsymbol{e} _ i + \boldsymbol{e}_{i+1})^T \boldsymbol{W}\right)$.</answer>
		<answer correct="true">Predicts the probability that word $w_j$ appears in a context window of the word $w_i$ as $\sigma(\boldsymbol{e} _ i^T \boldsymbol{W}_{\ast,j})$.</answer>
		<answer>Uses logistic regression to predict if two words can appear in a given context window, i.e., there is one output matrix $\boldsymbol{W}$ for each word. As a results of this, the skip-gram model has $|V|^3$ parameters where $|V|$ is the vocabulary size.</answer>
		<answer>Avoid using logistic regression because logistic regression only learns the parameter $\boldsymbol{W}$, but our goal is learning the embedding matrix $\boldsymbol{E}$.</answer>
	</question>

	<question>
		<text>A reasonable way how to use word embeddings for part-of-speech tagging would be:</text>

		<answer>Use word embeddings to retrieve similar words for training data augmentation.</answer>
		<answer>Use an average word embedding over a sentence to get the overall context and then add one-features that describe each specific word (mostly its prefixes and suffixes).</answer>
		<answer>Use only the embedding of the word being classified, so it is not influenced by the context.</answer>
		<answer correct="true">Use a sliding window over the sentence and predict the tag for the middle word.</answer>
	</question>
</quiz>
