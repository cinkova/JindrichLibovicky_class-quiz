# Large Language Models: Lecture 8 Quiz

## What is usually the most in-demand computational resource for fine-tuning LLM?
1. (X) Disk space
2. VRAM (GPU Memory)
3. Inference time
4. RAM (CPU Memory)

## How much GPU memory is approximately needed to fine-tune 10B parameter model (with Adam optimizer)?
1. 16 GB
2. 61 GB
3. (X) 160 GB
4. 610 GB

## LoRA is an algorithm for:
1. (X) Parameter efficient fine-tuning
2. Speeding-up inference
3. Quantization
4. Data compression

## Which of the following is **not** used in qLoRA?
1. Double quantization
2. Low rank adaptation
3. Paged optimization
4. (X) 1.58 bit parameters

## What is the Chinchilla-optimal token to parameter ratio in pre-training?
1. 1:1
2. 2:1
3. 10:1
4. (X) 20:1
