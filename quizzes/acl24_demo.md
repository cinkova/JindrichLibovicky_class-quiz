# Large Language Models: Quiz demo for the TeachNLP workshop at ACL 2024

## What is the objective for training generative LLMs?

1. Predicting the right token on a masked position inside a target sequence.
2. Predicting the right sequence of tokens given a prefix.
3. (X) Predicting the right token that follows a given prefix.


## How do token embeddings and position embeddings work together in a Transformer model?

1. Token embeddings are added to the model's output, and position embeddings are ignored.
2. Position embeddings replace token embeddings after the self-attention layer.
3. (X) Token embeddings provide the meaning of each token, and position embeddings provide information about the order of tokens.
4. Token embeddings are used during training, while position embeddings are used during inference.


## Which of the following is the formula for the cross-entropy loss?

1. $\max(0, 1 - \hat{y} \cdot y)$
2. $(y - \hat{y}^2)$
3. (X) $- \frac{1}{|\mathcal{D}|} \sum_{(x,y) \in \mathcal{D}} \log p_{\theta}(y|x)$


## What does it mean when a model is described as "autoregressive"?

1. The model predicts the regression coefficients automatically.
2. The model generates text by conditioning on the entire dataset.
3. (X) The model generates each token by conditioning on the previously generated tokens.
4. The model regresses towards the mean of the training data.


## Which of the following best describes "top-$k$ sampling"?

1. Sampling from the $k$ **most recent tokens** generated by the model.
2. Sampling from the **entire vocabulary**, then filtering out the top $k$ tokens.
3. (X) Sampling from the $k$ **most likely** next tokens at each step of the generation.
4. Sampling from the **least likely** $k$ tokens to increase output variability.


## How does beam search differ from greedy search?

1. (X) Beam search keeps multiple hypotheses at each step, while greedy search always picks the single best next token.
2. Beam search generates text faster than greedy search.
3. Beam search always produces more coherent text than greedy search.
4. Greedy search can backtrack to previous states, but beam search cannot.


## LoRA is an algorithm for:
1. (X) Parameter efficient fine-tuning
2. Speeding-up inference
3. Quantization
4. Data compression


## Ability to solve a task in one language after fine-tuning or instructing in another language is attributed to:

1. Forced inference
2. (X) Cross-lingual transfer
3. Reinforcement learning from human feedback
4. Quantization


## Which languages are the most prone to over-segmentation:

1. Low-resource, written with Latin script
2. High-resource, written with Latin script
3. (X) Low-resource, written with non-Latin script
4. High-resource, written with non-Latin script


## In natural language inference, what is "entailment"?

1. When one sentence logically contradicts another sentence.
2. When one sentence is less informative than another sentence.
3. (X) When one sentence logically follows from another sentence.
4. When one sentence is unrelated to another sentence.
